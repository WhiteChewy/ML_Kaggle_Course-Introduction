Недостаточно близкая и переизбыточная подгонка

Теперь когда у нас есть уверенный способ для измерения точности модели мы можем эксперементировать с альтернативными
моделями и увидеть какая дает наилучшие предсказания.

В scikit-learn у модели дерева есть множество опций. Самое главное это определение "глубины" дерева. Представленное
ниже дерево объективно мало.

                                                    В доме 2 спальни?
                                    ________________________________________________
                                   нет /                                   \ да
                                      /                                     \
                                Участок больше                        Участок больше
                                   8,5к фут^2                            11,5к фут^2
                              нет /          \ да                нет /             \ да
                        Возможная цена   Возможная цена     Возможная цена     Возможная цена
                            $146к             $188к              $170к             $233к

На практике иметь глубину дерева равной 10 не так уж и необычно. С тем как дерево становится глубже, датасет разделяется
все на большее количество групп с меньшим количеством данных в них. Если дерево имеет только 1 разделение - данные
разделяются только на 2 группы. Если эти данные тоже разделить, то глубина дерева будет 2, а колчество групп увеличится
до 4. И так когда мы будем иметь глубину дерева 10 - количество наших групп будет равно 2^10 что равняется 1024.

Далее разбираем на примере предсказания цен домов.
Когда мы разделяем дома на множество "листков" дерева это уменьшает общее количество домов в каждом "листке". "Листки"
с меньшим количеством домов в них будут гораздо точнее предсказывать цены домов в пределах этого листка, НО они могут
достаточно не точно предсказывать цены для новых данных. (Это связанно с тем что предсказания сделанны на маленькой 
группе домов)

Это явление называется "Переизбыточная подгонка", когда модель почти идеально работает на данных для тренировки, но плохо
отрабатывает на данных для оценки и других, новых,  данных.

Напротив, если глубана дерева будет 1 или 2, каждая группа домов будет очень разномастной. В результате предсказания 
будут очень не точными как на тренировочных данных, так и на проверочных.  Когда модель не справляется с задачей выявить
важные паттерны и различия в данных и из-за этого имеет низкую точность даже в данных для тренировки - это называется 
"недостаточная подгонка".

Существует несколько альтернатив для контроля за глубиной дерева. Но использовкание аргумента "max_leaf_nodes" в DecisionTreeRegressor
из модуля scikit-learn позволяет очень точно контролировать недостаточную или переизбыточную подгонку.